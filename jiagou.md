# 整体实验架构

## 1. 数据集

   使用的谷歌新闻数据集，新闻中的**对应描述语句**与**标题**来作为对应的语料，并且为了得到长句的子序列来得到parallel训练集来进行训练，提出一种方法来获取*Overcoming the Lack of Parallel Data in Sentence Compression*。  
   
   数据由三个部分组成：  
   i. 原句子  
   ii. 标题句子  
   iii. 由原句子子序列构成的压缩目标句  
   
   
# 2. 数据处理  
  主要讨论投入的数据集处理方式。  
#### 1. **分词部分**：
> 简单分词，使用空格来分就好了吧，那些what's之类的连接词可以看做一个词？  
> （看一下官方提供的词向量中有没有提供what's之类的词向量？有空总结一下数据预处理的方式）  

   
#### 2. **论文中的参数设置**： 
> - 一共包含10,000条训练数据对，其中8,000条来训练，1,000条作为交叉验证，1,000条来测试。  
> - 使用8,000最常见的词来作为字典大小（Tips:这8000个词涵盖了90%数量的词）。  
> - 不在字典中的词使用"unknow"来表示，并且在句子开头加上"sos"，在句子结尾加上"eos"。  
	
#### 3. **评判标准**  
> i. F1-score 召回率  
	ii. accuracy 句子准确率  
	iii. CRate压缩文件概率
	
# 3. 模型部分
#### 3.1 baseline

	
	
	
	   
   
   
   